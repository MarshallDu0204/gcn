Graph Convolution Network

数据预处理：
normalization
意义：对于度大的节点特征越来越大，而对于度小的节点却相反，这可能导致网络训练过程中梯度爆炸或者消失的问题
L=D^-0.5 * (A+I) * D^-0.5
作用: 增加节点本身的影响以及normalization

1）
输入输出

N: 节点数量
D: 每个节点初始特征数(one-hot编码)
K: 每个节点输出特征数

输入 邻接矩阵 A 表示节点之间关系 （N*N）
输入 节点特征矩阵 X 表示每个输入节点的特征 (N*D)

输出 特征矩阵Z (N*K) 表示每个节点的输出特征

多层GCN嵌套时，上层GCN的输出是下层GCN节点特征的输入，临界矩阵不变

GCN运算逻辑：
数据预处理后(normalization)
weight 的维度为(D,K) (input_dim,out_dim)
输入的node feature matmul weight  (matmul(input_feature,weight))
再将邻接矩阵 matmul 26行输出的结果  (matmul(adjacency,line26_result))
(再加上bias, optional)

实际意义对图进行卷积后每个节点都学习到了neighbourhood节点的信息 每次卷积都是相邻节点特征进行学习，
所以卷积次数决定了学习邻居节点的广度 平均卷积2-3次左右效果最好

可以对每个输出节点的特征通过softmax函数进行分类 （节点分类）e.g:
图 adj_matrix(2708,2708), feature(2708,1433)(after onthot), label(2708,7)(after onthot)
adj_matrix,feature -->GCN layer1(relu)-->  (2708,128) 
--> GCN layer2(softmax) -->(2708,7)


延申：

GNN的深度(层数)
GNN的最佳层数和邻居矩阵A的稀疏程度有关系，当图的稀疏度较低时,很快会出现过平滑现象.

GCN 层数高时性能会下降 (远距离节点信息传播能力下降)

解决模型: PPNP/AAPNP 论文：https://arxiv.org/pdf/1810.05997.pdf

PPNP/AAPNP的层数应设置为图的平均最短路径(average shortest path)   

average shortest path 计算方法： https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.shortest_paths.generic.average_shortest_path_length.html